{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942b3c96",
   "metadata": {},
   "source": [
    "# PubMed Baseline Search for Quaternary Ammonium Compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c724f89",
   "metadata": {},
   "source": [
    "Before running this notebook, ensure the PubMed Baseline database has been downloaded locally. \n",
    "This code rapidly downloads this using parallel processing: https://github.com/ScottCoffin/pubmed-baseline-mirror"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc9f8ef",
   "metadata": {},
   "source": [
    "This notebook:\n",
    "1. Unpacks PubMed `.xml.gz` baseline files.\n",
    "2. Loads a search term list from a `.txt` file.\n",
    "3. Searches for matches using regular expressions.\n",
    "4. Optionally extracts article metadata.\n",
    "5. Saves results to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db24d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a685edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "source_dir = Path(\"pubmed-baseline-mirror/data/downloads\")\n",
    "output_dir = Path(\"pubmed-baseline-mirror/data/unpacked\")\n",
    "search_terms_file = Path(\"search_terms.txt\")  # path to your .txt file with search terms (one per line)\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321077ff",
   "metadata": {},
   "source": [
    "## Unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70de0b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unpacking .xml.gz files: 100%|██████████| 1274/1274 [16:41<00:00,  1.27it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Unpack .xml.gz files with progress reporting\n",
    "gz_files = [f for f in source_dir.iterdir() if f.suffix == \".gz\"]\n",
    "\n",
    "for fname in tqdm(gz_files, desc=\"Unpacking .xml.gz files\"):\n",
    "    target_path = output_dir / fname.with_suffix('').name\n",
    "    if not target_path.exists():\n",
    "        with gzip.open(fname, 'rb') as f_in:\n",
    "            with open(target_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb73fb",
   "metadata": {},
   "source": [
    "## Load Search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b1c7a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 search terms.\n"
     ]
    }
   ],
   "source": [
    "# Load search terms from .txt file and build regex\n",
    "with open(search_terms_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    query_terms = [line.strip() for line in f if line.strip() and not line.strip().startswith(\"#\")]\n",
    "\n",
    "search_pattern = re.compile(r\"|\".join(re.escape(term) for term in query_terms), re.IGNORECASE)\n",
    "print(f\"Loaded {len(query_terms)} search terms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c44402",
   "metadata": {},
   "source": [
    "## Run Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Threaded search:  55%|█████▍    | 696/1274 [09:39<23:06,  2.40s/it]  "
     ]
    }
   ],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load search terms\n",
    "with open(search_terms_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    query_terms = [line.strip() for line in f if line.strip() and not line.startswith(\"#\")]\n",
    "search_pattern = re.compile(\"|\".join(re.escape(term) for term in query_terms), re.IGNORECASE)\n",
    "\n",
    "# Collect XML files\n",
    "xml_files = [str(f) for f in output_dir.glob(\"*.xml\")]\n",
    "\n",
    "def search_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            contents = f.read()\n",
    "            if search_pattern.search(contents):\n",
    "                return filepath\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Thread pool (use ~4–8 threads depending on system)\n",
    "with ThreadPool(8) as pool:\n",
    "    results = list(tqdm(pool.imap(search_file, xml_files), total=len(xml_files), desc=\"Threaded search\"))\n",
    "\n",
    "# Filter matched files\n",
    "matched_files = [f for f in results if f]\n",
    "print(f\"✅ Found {len(matched_files)} matching files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: extract article metadata\n",
    "articles = []\n",
    "\n",
    "for fname in tqdm(matched_files, desc=\"Extracting article data\"):\n",
    "    with open(output_dir / fname, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        for article in soup.find_all(\"pubmedarticle\"):\n",
    "            title = article.find(\"articletitle\")\n",
    "            abstract = article.find(\"abstract\")\n",
    "            pmid = article.find(\"pmid\")\n",
    "            articles.append({\n",
    "                \"pmid\": pmid.text if pmid else None,\n",
    "                \"title\": title.text if title else \"\",\n",
    "                \"abstract\": abstract.text if abstract else \"\"\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(articles)\n",
    "df.to_csv(\"pubmed_qac_matches.csv\", index=False)\n",
    "print(f\"Saved {len(df)} matched articles to pubmed_qac_matches.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ecbcb",
   "metadata": {},
   "source": [
    "# 🔬 Advanced Analysis & Insights\n",
    "\n",
    "Let's dive deep into the data with comprehensive analytical capabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7164f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages for advanced analysis\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✅ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"❌ Failed to install {package}\")\n",
    "\n",
    "# Install packages for advanced analysis\n",
    "packages = [\n",
    "    \"wordcloud\", \"plotly\", \"networkx\", \"scikit-learn\", \n",
    "    \"textblob\", \"seaborn\", \"matplotlib\", \"nltk\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e52de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for advanced analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from wordcloud import WordCloud\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Enhanced Metadata Extraction\n",
    "print(\"🔍 Extracting enhanced metadata from matched articles...\")\n",
    "\n",
    "enhanced_articles = []\n",
    "qac_mentions = []\n",
    "author_data = []\n",
    "journal_data = []\n",
    "\n",
    "for fname in tqdm(matched_files, desc=\"Enhanced extraction\"):\n",
    "    filepath = Path(fname) if isinstance(fname, str) else fname\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "            \n",
    "            for article in soup.find_all(\"pubmedarticle\"):\n",
    "                # Basic info\n",
    "                title_elem = article.find(\"articletitle\")\n",
    "                abstract_elem = article.find(\"abstract\")\n",
    "                pmid_elem = article.find(\"pmid\")\n",
    "                \n",
    "                title = title_elem.text if title_elem else \"\"\n",
    "                abstract = abstract_elem.text if abstract_elem else \"\"\n",
    "                pmid = pmid_elem.text if pmid_elem else None\n",
    "                \n",
    "                if not title and not abstract:\n",
    "                    continue\n",
    "                \n",
    "                # Enhanced metadata\n",
    "                year = None\n",
    "                journal = \"\"\n",
    "                authors = []\n",
    "                keywords = []\n",
    "                mesh_terms = []\n",
    "                \n",
    "                # Extract publication year\n",
    "                pub_date = article.find(\"pubdate\") or article.find(\"articledate\")\n",
    "                if pub_date:\n",
    "                    year_elem = pub_date.find(\"year\")\n",
    "                    if year_elem:\n",
    "                        try:\n",
    "                            year = int(year_elem.text)\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                \n",
    "                # Extract journal\n",
    "                journal_elem = article.find(\"journal\")\n",
    "                if journal_elem:\n",
    "                    title_elem = journal_elem.find(\"title\")\n",
    "                    if title_elem:\n",
    "                        journal = title_elem.text\n",
    "                \n",
    "                # Extract authors\n",
    "                author_list = article.find(\"authorlist\")\n",
    "                if author_list:\n",
    "                    for author in author_list.find_all(\"author\"):\n",
    "                        lastname = author.find(\"lastname\")\n",
    "                        forename = author.find(\"forename\")\n",
    "                        if lastname:\n",
    "                            name = lastname.text\n",
    "                            if forename:\n",
    "                                name = f\"{forename.text} {name}\"\n",
    "                            authors.append(name)\n",
    "                            author_data.append({\n",
    "                                'pmid': pmid,\n",
    "                                'author': name,\n",
    "                                'year': year\n",
    "                            })\n",
    "                \n",
    "                # Extract keywords\n",
    "                keyword_list = article.find(\"keywordlist\")\n",
    "                if keyword_list:\n",
    "                    for keyword in keyword_list.find_all(\"keyword\"):\n",
    "                        keywords.append(keyword.text)\n",
    "                \n",
    "                # Extract MeSH terms\n",
    "                mesh_list = article.find(\"meshheadinglist\")\n",
    "                if mesh_list:\n",
    "                    for mesh in mesh_list.find_all(\"meshheading\"):\n",
    "                        descriptor = mesh.find(\"descriptorname\")\n",
    "                        if descriptor:\n",
    "                            mesh_terms.append(descriptor.text)\n",
    "                \n",
    "                # Find QAC mentions in text\n",
    "                full_text = f\"{title} {abstract}\".lower()\n",
    "                qac_matches = search_pattern.findall(full_text)\n",
    "                \n",
    "                for match in qac_matches:\n",
    "                    qac_mentions.append({\n",
    "                        'pmid': pmid,\n",
    "                        'compound': match,\n",
    "                        'year': year,\n",
    "                        'journal': journal\n",
    "                    })\n",
    "                \n",
    "                # Store enhanced article data\n",
    "                enhanced_articles.append({\n",
    "                    \"pmid\": pmid,\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"year\": year,\n",
    "                    \"journal\": journal,\n",
    "                    \"authors\": \"; \".join(authors),\n",
    "                    \"author_count\": len(authors),\n",
    "                    \"keywords\": \"; \".join(keywords),\n",
    "                    \"mesh_terms\": \"; \".join(mesh_terms),\n",
    "                    \"text_length\": len(full_text),\n",
    "                    \"qac_mention_count\": len(qac_matches)\n",
    "                })\n",
    "                \n",
    "                journal_data.append({\n",
    "                    'pmid': pmid,\n",
    "                    'journal': journal,\n",
    "                    'year': year\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Create enhanced dataframes\n",
    "df_enhanced = pd.DataFrame(enhanced_articles)\n",
    "df_qac_mentions = pd.DataFrame(qac_mentions)\n",
    "df_authors = pd.DataFrame(author_data)\n",
    "df_journals = pd.DataFrame(journal_data)\n",
    "\n",
    "print(f\"✅ Enhanced extraction complete!\")\n",
    "print(f\"📄 Articles: {len(df_enhanced)}\")\n",
    "print(f\"💊 QAC mentions: {len(df_qac_mentions)}\")\n",
    "print(f\"👥 Author entries: {len(df_authors)}\")\n",
    "print(f\"📰 Journal entries: {len(df_journals)}\")\n",
    "\n",
    "# Display sample of enhanced data\n",
    "print(\"\\n📋 Sample of enhanced data:\")\n",
    "print(df_enhanced[['pmid', 'title', 'year', 'journal', 'author_count', 'qac_mention_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60adc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Temporal Analysis & Publication Trends\n",
    "\n",
    "# Filter data with valid years\n",
    "df_with_years = df_enhanced.dropna(subset=['year']).copy()\n",
    "df_with_years = df_with_years[(df_with_years['year'] >= 1950) & (df_with_years['year'] <= 2025)]\n",
    "\n",
    "if len(df_with_years) > 0:\n",
    "    # Publication trends over time\n",
    "    yearly_counts = df_with_years.groupby('year').size().reset_index(name='count')\n",
    "    \n",
    "    # Create interactive time series plot\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Publications Over Time', 'QAC Mentions Over Time', \n",
    "                       'Average Authors per Paper', 'Cumulative Publications'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Publications over time\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=yearly_counts['year'], y=yearly_counts['count'],\n",
    "                  mode='lines+markers', name='Publications',\n",
    "                  line=dict(color='blue', width=3)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # QAC mentions over time\n",
    "    if len(df_qac_mentions) > 0:\n",
    "        qac_yearly = df_qac_mentions.dropna(subset=['year']).groupby('year').size().reset_index(name='mentions')\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=qac_yearly['year'], y=qac_yearly['mentions'],\n",
    "                      mode='lines+markers', name='QAC Mentions',\n",
    "                      line=dict(color='red', width=3)),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Average authors per paper\n",
    "    author_trends = df_with_years.groupby('year')['author_count'].mean().reset_index()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=author_trends['year'], y=author_trends['author_count'],\n",
    "                  mode='lines+markers', name='Avg Authors',\n",
    "                  line=dict(color='green', width=3)),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Cumulative publications\n",
    "    yearly_counts['cumulative'] = yearly_counts['count'].cumsum()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=yearly_counts['year'], y=yearly_counts['cumulative'],\n",
    "                  mode='lines+markers', name='Cumulative',\n",
    "                  line=dict(color='purple', width=3)),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"📊 QAC Research Temporal Analysis\", showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "    # Growth analysis\n",
    "    recent_years = yearly_counts[yearly_counts['year'] >= 2010]\n",
    "    if len(recent_years) > 5:\n",
    "        growth_rate = np.polyfit(recent_years['year'], recent_years['count'], 1)[0]\n",
    "        print(f\"📈 Publication growth rate (2010+): {growth_rate:.2f} papers/year\")\n",
    "    \n",
    "    # Peak years\n",
    "    top_years = yearly_counts.nlargest(5, 'count')\n",
    "    print(f\"\\n🔥 Top 5 publication years:\")\n",
    "    for _, row in top_years.iterrows():\n",
    "        print(f\"   {int(row['year'])}: {row['count']} papers\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️  No temporal data available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b249c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💊 QAC Compound Analysis & Chemical Profiling\n",
    "\n",
    "print(\"🧪 Analyzing QAC compound mentions...\")\n",
    "\n",
    "if len(df_qac_mentions) > 0:\n",
    "    # Most frequently mentioned compounds\n",
    "    compound_counts = df_qac_mentions['compound'].value_counts().head(20)\n",
    "    \n",
    "    # Create compound analysis visualization\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Top QAC Compounds', 'Compound Trends Over Time', \n",
    "                       'Journal Distribution', 'Compound Categories'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"pie\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # Top compounds bar chart\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=compound_counts.head(10).values, y=compound_counts.head(10).index,\n",
    "               orientation='h', name='Mentions',\n",
    "               marker_color='lightblue'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Compound trends over time (top 5 compounds)\n",
    "    top_compounds = compound_counts.head(5).index\n",
    "    for i, compound in enumerate(top_compounds):\n",
    "        compound_data = df_qac_mentions[df_qac_mentions['compound'] == compound]\n",
    "        if 'year' in compound_data.columns:\n",
    "            yearly_data = compound_data.dropna(subset=['year']).groupby('year').size().reset_index(name='count')\n",
    "            if len(yearly_data) > 0:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=yearly_data['year'], y=yearly_data['count'],\n",
    "                              mode='lines+markers', name=compound[:20]),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "    \n",
    "    # Journal distribution for QAC research\n",
    "    journal_qac_counts = df_qac_mentions['journal'].value_counts().head(10)\n",
    "    if len(journal_qac_counts) > 0:\n",
    "        fig.add_trace(\n",
    "            go.Pie(labels=journal_qac_counts.index, values=journal_qac_counts.values,\n",
    "                   name=\"Journals\"),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Categorize compounds\n",
    "    categories = {\n",
    "        'Antimicrobial': ['benzalkonium', 'cetylpyridinium', 'benzethonium', 'didecyldimethylammonium'],\n",
    "        'Industrial': ['polyquaternium', 'diallyldimethylammonium', 'quaternium'],\n",
    "        'Research_Tools': ['tetramethylammonium', 'tetraethylammonium', 'tetrabutylammonium'],\n",
    "        'Herbicides': ['paraquat', 'diquat'],\n",
    "        'Dyes': ['malachite green', 'brilliant green', 'basic blue']\n",
    "    }\n",
    "    \n",
    "    category_counts = defaultdict(int)\n",
    "    for compound in df_qac_mentions['compound']:\n",
    "        compound_lower = compound.lower()\n",
    "        categorized = False\n",
    "        for category, keywords in categories.items():\n",
    "            if any(keyword in compound_lower for keyword in keywords):\n",
    "                category_counts[category] += 1\n",
    "                categorized = True\n",
    "                break\n",
    "        if not categorized:\n",
    "            category_counts['Other'] += 1\n",
    "    \n",
    "    if category_counts:\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=list(category_counts.keys()), y=list(category_counts.values()),\n",
    "                   name='Categories', marker_color='orange'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=1000, title_text=\"💊 QAC Compound Analysis Dashboard\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"\\n📊 Compound Statistics:\")\n",
    "    print(f\"   Total unique compounds: {df_qac_mentions['compound'].nunique()}\")\n",
    "    print(f\"   Total mentions: {len(df_qac_mentions)}\")\n",
    "    print(f\"   Average mentions per compound: {len(df_qac_mentions) / df_qac_mentions['compound'].nunique():.1f}\")\n",
    "    \n",
    "    print(f\"\\n🔝 Top 10 compounds:\")\n",
    "    for i, (compound, count) in enumerate(compound_counts.head(10).items(), 1):\n",
    "        print(f\"   {i:2d}. {compound}: {count} mentions\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  No QAC mention data available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce73c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📰 Journal & Publication Venue Analysis\n",
    "\n",
    "print(\"📚 Analyzing publication venues and journal patterns...\")\n",
    "\n",
    "if len(df_enhanced) > 0:\n",
    "    # Clean and analyze journals\n",
    "    df_journals_clean = df_enhanced[df_enhanced['journal'].notna() & (df_enhanced['journal'] != '')].copy()\n",
    "    \n",
    "    if len(df_journals_clean) > 0:\n",
    "        journal_stats = df_journals_clean.groupby('journal').agg({\n",
    "            'pmid': 'count',\n",
    "            'year': ['min', 'max', 'mean'],\n",
    "            'author_count': 'mean',\n",
    "            'qac_mention_count': 'sum'\n",
    "        }).round(2)\n",
    "        \n",
    "        journal_stats.columns = ['articles', 'first_year', 'last_year', 'avg_year', 'avg_authors', 'total_qac_mentions']\n",
    "        journal_stats = journal_stats.reset_index().sort_values('articles', ascending=False)\n",
    "        \n",
    "        # Create journal analysis dashboard\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Top Journals by Article Count', 'Journal Timeline', \n",
    "                           'Journal Impact (QAC Focus)', 'Authorship Patterns'),\n",
    "            specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "                   [{\"type\": \"scatter\"}, {\"type\": \"box\"}]]\n",
    "        )\n",
    "        \n",
    "        # Top journals\n",
    "        top_journals = journal_stats.head(15)\n",
    "        fig.add_trace(\n",
    "            go.Bar(y=top_journals['journal'], x=top_journals['articles'],\n",
    "                   orientation='h', name='Articles',\n",
    "                   marker_color='steelblue'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Journal timeline (when journals started/ended publishing QAC research)\n",
    "        for i, journal in enumerate(top_journals.head(10)['journal']):\n",
    "            journal_data = journal_stats[journal_stats['journal'] == journal].iloc[0]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=[journal_data['first_year'], journal_data['last_year']], \n",
    "                          y=[journal[:30], journal[:30]],\n",
    "                          mode='lines+markers', \n",
    "                          name=journal[:20],\n",
    "                          line=dict(width=4)),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # QAC focus vs article count\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=journal_stats['articles'], y=journal_stats['total_qac_mentions'],\n",
    "                      mode='markers', name='Journals',\n",
    "                      text=journal_stats['journal'],\n",
    "                      hovertemplate='%{text}<br>Articles: %{x}<br>QAC Mentions: %{y}',\n",
    "                      marker=dict(size=8, opacity=0.6)),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Author patterns by journal type\n",
    "        journal_types = []\n",
    "        for journal in df_journals_clean['journal']:\n",
    "            journal_lower = journal.lower()\n",
    "            if any(term in journal_lower for term in ['toxicol', 'environ', 'ecotoxicol']):\n",
    "                journal_types.append('Environmental/Toxicology')\n",
    "            elif any(term in journal_lower for term in ['chem', 'analyt', 'chromatogr']):\n",
    "                journal_types.append('Chemistry/Analytical')\n",
    "            elif any(term in journal_lower for term in ['microbiol', 'antimicrob', 'infect']):\n",
    "                journal_types.append('Microbiology/Infectious')\n",
    "            elif any(term in journal_lower for term in ['food', 'safet']):\n",
    "                journal_types.append('Food Safety')\n",
    "            else:\n",
    "                journal_types.append('Other')\n",
    "        \n",
    "        df_journals_clean['journal_type'] = journal_types\n",
    "        \n",
    "        for jtype in df_journals_clean['journal_type'].unique():\n",
    "            type_data = df_journals_clean[df_journals_clean['journal_type'] == jtype]['author_count']\n",
    "            fig.add_trace(\n",
    "                go.Box(y=type_data, name=jtype),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(height=1000, title_text=\"📰 Journal & Publication Analysis Dashboard\")\n",
    "        fig.show()\n",
    "        \n",
    "        # Journal insights\n",
    "        print(f\"\\\\n📈 Journal Statistics:\")\n",
    "        print(f\"   Total journals: {len(journal_stats)}\")\n",
    "        print(f\"   Journals with 10+ articles: {len(journal_stats[journal_stats['articles'] >= 10])}\")\n",
    "        print(f\"   Most productive journal: {journal_stats.iloc[0]['journal']} ({journal_stats.iloc[0]['articles']} articles)\\\")\\n        \n",
    "        print(f\\\"\\\\n🏆 Top 10 journals:\\\")\n",
    "        for i, (_, row) in enumerate(journal_stats.head(10).iterrows(), 1):\n",
    "            print(f\\\"   {i:2d}. {row['journal'][:50]}: {row['articles']} articles ({row['first_year']:.0f}-{row['last_year']:.0f})\\\")\n",
    "        \n",
    "        # Journal type analysis\n",
    "        type_stats = df_journals_clean.groupby('journal_type').agg({\n",
    "            'pmid': 'count',\n",
    "            'author_count': 'mean',\n",
    "            'qac_mention_count': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        print(f\\\"\\\\n📚 Journal type analysis:\\\")\n",
    "        for jtype, stats in type_stats.iterrows():\n",
    "            print(f\\\"   {jtype}: {stats['pmid']} articles, avg {stats['author_count']} authors, {stats['qac_mention_count']} QAC mentions\\\")\n",
    "            \n",
    "    else:\n",
    "        print(\\\"⚠️  No journal data available for analysis\\\")\n",
    "else:\n",
    "    print(\\\"⚠️  No article data available for analysis\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Text Mining & Topic Modeling\n",
    "\n",
    "print(\"📝 Performing advanced text analysis...\")\n",
    "\n",
    "if len(df_enhanced) > 0:\n",
    "    # Combine title and abstract for analysis\n",
    "    df_text = df_enhanced.copy()\n",
    "    df_text['full_text'] = (df_text['title'].fillna('') + ' ' + df_text['abstract'].fillna('')).str.strip()\n",
    "    df_text = df_text[df_text['full_text'].str.len() > 10]  # Filter out empty texts\n",
    "    \n",
    "    if len(df_text) > 0:\n",
    "        print(f\\\"Analyzing {len(df_text)} articles with sufficient text...\\\")\\n        \n",
    "        # Word cloud generation\n",
    "        all_text = ' '.join(df_text['full_text'])\n",
    "        \n",
    "        # Create word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, \n",
    "                             background_color='white',\n",
    "                             max_words=100,\n",
    "                             colormap='viridis',\n",
    "                             stopwords={'the', 'and', 'or', 'of', 'in', 'to', 'for', 'with', 'by', 'from', 'at', 'on', 'an', 'as', 'are', 'was', 'were', 'been', 'is', 'this', 'that', 'these', 'those', 'study', 'studies', 'using', 'used', 'analysis', 'method', 'methods', 'results', 'conclusion', 'conclusions'}).generate(all_text)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('🌟 Word Cloud of QAC Research Literature', size=16, pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # TF-IDF Analysis for key terms\n",
    "        print(\\\"\\\\n🔬 Extracting key terms using TF-IDF...\\\")\n",
    "        vectorizer = TfidfVectorizer(max_features=1000, \n",
    "                                   stop_words='english',\n",
    "                                   ngram_range=(1, 3),\n",
    "                                   min_df=2,\n",
    "                                   max_df=0.8)\n",
    "        \n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(df_text['full_text'])\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Get top terms by TF-IDF score\n",
    "            mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)\n",
    "            top_indices = mean_scores.argsort()[::-1][:30]\n",
    "            top_terms = [(feature_names[i], mean_scores[i]) for i in top_indices]\n",
    "            \n",
    "            # Visualize top terms\n",
    "            terms, scores = zip(*top_terms[:20])\n",
    "            \n",
    "            fig = go.Figure(data=go.Bar(\n",
    "                x=list(scores),\n",
    "                y=list(terms),\n",
    "                orientation='h',\n",
    "                marker_color='lightcoral'\n",
    "            ))\n",
    "            fig.update_layout(\n",
    "                title=\\\"🎯 Top 20 Terms by TF-IDF Score\\\",\n",
    "                xaxis_title=\\\"TF-IDF Score\\\",\n",
    "                height=600\n",
    "            )\n",
    "            fig.show()\n",
    "            \n",
    "            print(f\\\"\\\\n🔝 Top terms in QAC literature:\\\")\n",
    "            for i, (term, score) in enumerate(top_terms[:15], 1):\n",
    "                print(f\\\"   {i:2d}. {term}: {score:.4f}\\\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\\\"TF-IDF analysis failed: {e}\\\")\n",
    "        \n",
    "        # Topic Modeling with LDA\n",
    "        if len(df_text) >= 10:  # Need sufficient documents for LDA\n",
    "            print(f\\\"\\\\n🎭 Performing topic modeling...\\\")\n",
    "            try:\n",
    "                # Use CountVectorizer for LDA\n",
    "                count_vectorizer = CountVectorizer(max_features=500,\n",
    "                                                 stop_words='english',\n",
    "                                                 min_df=2,\n",
    "                                                 max_df=0.8)\n",
    "                count_matrix = count_vectorizer.fit_transform(df_text['full_text'])\n",
    "                \n",
    "                # Fit LDA model\n",
    "                n_topics = min(8, len(df_text) // 5)  # Adaptive number of topics\n",
    "                lda = LatentDirichletAllocation(n_components=n_topics, \n",
    "                                              random_state=42,\n",
    "                                              max_iter=10)\n",
    "                lda.fit(count_matrix)\n",
    "                \n",
    "                # Extract and display topics\n",
    "                feature_names = count_vectorizer.get_feature_names_out()\n",
    "                \n",
    "                topics_data = []\n",
    "                for topic_idx, topic in enumerate(lda.components_):\n",
    "                    top_words_idx = topic.argsort()[::-1][:10]\n",
    "                    top_words = [feature_names[i] for i in top_words_idx]\n",
    "                    topics_data.append({\n",
    "                        'topic': f'Topic {topic_idx + 1}',\n",
    "                        'words': ', '.join(top_words),\n",
    "                        'weight': topic[top_words_idx[0]]\n",
    "                    })\n",
    "                \n",
    "                topics_df = pd.DataFrame(topics_data)\n",
    "                \n",
    "                print(f\\\"\\\\n🎭 Discovered {n_topics} topics:\\\")\n",
    "                for i, row in topics_df.iterrows():\n",
    "                    print(f\\\"   {row['topic']}: {row['words']}\\\")\n",
    "                \n",
    "                # Visualize topic weights\n",
    "                fig = go.Figure(data=go.Bar(\n",
    "                    x=topics_df['topic'],\n",
    "                    y=topics_df['weight'],\n",
    "                    marker_color='skyblue'\n",
    "                ))\n",
    "                fig.update_layout(\n",
    "                    title=\\\"📊 Topic Weights in QAC Literature\\\",\n",
    "                    xaxis_title=\\\"Topics\\\",\n",
    "                    yaxis_title=\\\"Weight\\\"\n",
    "                )\n",
    "                fig.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\\\"Topic modeling failed: {e}\\\")\n",
    "        \n",
    "        # Sentiment Analysis\n",
    "        print(f\\\"\\\\n😊 Analyzing sentiment...\\\")\n",
    "        sentiments = []\n",
    "        for text in df_text['full_text'].head(100):  # Sample for speed\n",
    "            try:\n",
    "                blob = TextBlob(text)\n",
    "                sentiments.append({\n",
    "                    'polarity': blob.sentiment.polarity,\n",
    "                    'subjectivity': blob.sentiment.subjectivity\n",
    "                })\n",
    "            except:\n",
    "                sentiments.append({'polarity': 0, 'subjectivity': 0})\n",
    "        \n",
    "        if sentiments:\n",
    "            sentiment_df = pd.DataFrame(sentiments)\n",
    "            \n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=sentiment_df['polarity'],\n",
    "                y=sentiment_df['subjectivity'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=sentiment_df['polarity'],\n",
    "                    colorscale='RdYlBu',\n",
    "                    showscale=True,\n",
    "                    colorbar=dict(title=\\\"Polarity\\\")\n",
    "                ),\n",
    "                text=[f'Polarity: {p:.2f}<br>Subjectivity: {s:.2f}' \n",
    "                      for p, s in zip(sentiment_df['polarity'], sentiment_df['subjectivity'])],\n",
    "                hovertemplate='%{text}<extra></extra>'\n",
    "            ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=\\\"🎭 Sentiment Analysis of QAC Literature\\\",\n",
    "                xaxis_title=\\\"Polarity (Negative ← → Positive)\\\",\n",
    "                yaxis_title=\\\"Subjectivity (Objective ← → Subjective)\\\",\n",
    "                width=700, height=500\n",
    "            )\n",
    "            fig.show()\n",
    "            \n",
    "            avg_polarity = sentiment_df['polarity'].mean()\n",
    "            avg_subjectivity = sentiment_df['subjectivity'].mean()\n",
    "            print(f\\\"   Average polarity: {avg_polarity:.3f} (neutral: 0, positive: >0, negative: <0)\\\")\n",
    "            print(f\\\"   Average subjectivity: {avg_subjectivity:.3f} (objective: 0, subjective: 1)\\\")\n",
    "            \n",
    "    else:\n",
    "        print(\\\"⚠️  No sufficient text data available for analysis\\\")\n",
    "else:\n",
    "    print(\\\"⚠️  No article data available for text analysis\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c3323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🕸️ Research Collaboration Network Analysis\n",
    "\n",
    "print(\"🤝 Analyzing research collaboration networks...\")\n",
    "\n",
    "if len(df_authors) > 0:\n",
    "    # Author collaboration analysis\n",
    "    author_stats = df_authors.groupby('author').agg({\n",
    "        'pmid': 'nunique',\n",
    "        'year': ['min', 'max']\n",
    "    }).round(0)\n",
    "    author_stats.columns = ['papers', 'first_year', 'last_year']\n",
    "    author_stats = author_stats.reset_index().sort_values('papers', ascending=False)\n",
    "    \n",
    "    # Create author network\n",
    "    print(f\\\"Building collaboration network from {len(df_authors)} author entries...\\\")\n",
    "    \n",
    "    # Group by paper to find collaborations\n",
    "    paper_authors = df_authors.groupby('pmid')['author'].apply(list).reset_index()\n",
    "    paper_authors = paper_authors[paper_authors['author'].apply(len) > 1]  # Only multi-author papers\n",
    "    \n",
    "    if len(paper_authors) > 0:\n",
    "        # Build collaboration network\n",
    "        G = nx.Graph()\n",
    "        collaboration_count = defaultdict(int)\n",
    "        \n",
    "        for _, row in paper_authors.iterrows():\n",
    "            authors = row['author']\n",
    "            # Add edges between all pairs of authors on the same paper\n",
    "            for i in range(len(authors)):\n",
    "                for j in range(i+1, len(authors)):\n",
    "                    author1, author2 = sorted([authors[i], authors[j]])\n",
    "                    G.add_edge(author1, author2)\n",
    "                    collaboration_count[(author1, author2)] += 1\n",
    "        \n",
    "        if len(G.nodes()) > 0:\n",
    "            print(f\\\"Network stats: {len(G.nodes())} authors, {len(G.edges())} collaborations\\\")\n",
    "            \n",
    "            # Network metrics\n",
    "            try:\n",
    "                # Calculate centrality measures for top authors\n",
    "                top_authors = author_stats.head(50)['author'].tolist()\n",
    "                subgraph = G.subgraph([a for a in top_authors if a in G.nodes()])\n",
    "                \n",
    "                if len(subgraph.nodes()) > 0:\n",
    "                    degree_centrality = nx.degree_centrality(subgraph)\n",
    "                    betweenness_centrality = nx.betweenness_centrality(subgraph)\n",
    "                    closeness_centrality = nx.closeness_centrality(subgraph)\n",
    "                    \n",
    "                    # Create network analysis dataframe\n",
    "                    network_stats = []\n",
    "                    for author in subgraph.nodes():\n",
    "                        network_stats.append({\n",
    "                            'author': author,\n",
    "                            'degree_centrality': degree_centrality.get(author, 0),\n",
    "                            'betweenness_centrality': betweenness_centrality.get(author, 0),\n",
    "                            'closeness_centrality': closeness_centrality.get(author, 0),\n",
    "                            'papers': author_stats[author_stats['author'] == author]['papers'].iloc[0] if not author_stats[author_stats['author'] == author].empty else 0\n",
    "                        })\n",
    "                    \n",
    "                    network_df = pd.DataFrame(network_stats).sort_values('degree_centrality', ascending=False)\n",
    "                    \n",
    "                    # Visualize network metrics\n",
    "                    fig = make_subplots(\n",
    "                        rows=2, cols=2,\n",
    "                        subplot_titles=('Author Productivity', 'Degree Centrality', \n",
    "                                       'Betweenness Centrality', 'Network Overview'),\n",
    "                        specs=[[{\\\"type\\\": \\\"bar\\\"}, {\\\"type\\\": \\\"bar\\\"}],\n",
    "                               [{\\\"type\\\": \\\"bar\\\"}, {\\\"type\\\": \\\"scatter\\\"}]]\n",
    "                    )\n",
    "                    \n",
    "                    # Author productivity\n",
    "                    top_productive = author_stats.head(15)\n",
    "                    fig.add_trace(\n",
    "                        go.Bar(y=top_productive['author'], x=top_productive['papers'],\n",
    "                               orientation='h', name='Papers',\n",
    "                               marker_color='lightblue'),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "                    \n",
    "                    # Degree centrality\n",
    "                    top_central = network_df.head(15)\n",
    "                    fig.add_trace(\n",
    "                        go.Bar(y=top_central['author'], x=top_central['degree_centrality'],\n",
    "                               orientation='h', name='Centrality',\n",
    "                               marker_color='lightcoral'),\n",
    "                        row=1, col=2\n",
    "                    )\n",
    "                    \n",
    "                    # Betweenness centrality\n",
    "                    top_between = network_df.nlargest(15, 'betweenness_centrality')\n",
    "                    fig.add_trace(\n",
    "                        go.Bar(y=top_between['author'], x=top_between['betweenness_centrality'],\n",
    "                               orientation='h', name='Betweenness',\n",
    "                               marker_color='lightgreen'),\n",
    "                        row=2, col=1\n",
    "                    )\n",
    "                    \n",
    "                    # Network overview scatter\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(x=network_df['papers'], y=network_df['degree_centrality'],\n",
    "                                  mode='markers', name='Authors',\n",
    "                                  text=network_df['author'],\n",
    "                                  hovertemplate='%{text}<br>Papers: %{x}<br>Centrality: %{y:.3f}',\n",
    "                                  marker=dict(size=8, opacity=0.6, color='purple')),\n",
    "                        row=2, col=2\n",
    "                    )\n",
    "                    \n",
    "                    fig.update_layout(height=1000, title_text=\\\"🕸️ Research Collaboration Network Analysis\\\")\n",
    "                    fig.show()\n",
    "                    \n",
    "                    # Print network insights\n",
    "                    print(f\\\"\\\\n🌟 Top collaborative authors (by degree centrality):\\\")\n",
    "                    for i, (_, row) in enumerate(network_df.head(10).iterrows(), 1):\n",
    "                        print(f\\\"   {i:2d}. {row['author']}: {row['degree_centrality']:.3f} centrality, {row['papers']} papers\\\")\n",
    "                    \n",
    "                    print(f\\\"\\\\n🌉 Key bridge authors (by betweenness centrality):\\\")\n",
    "                    bridge_authors = network_df.nlargest(5, 'betweenness_centrality')\n",
    "                    for i, (_, row) in enumerate(bridge_authors.iterrows(), 1):\n",
    "                        print(f\\\"   {i}. {row['author']}: {row['betweenness_centrality']:.3f} betweenness\\\")\n",
    "                    \n",
    "                    # Collaboration strength analysis\n",
    "                    strong_collaborations = [(pair, count) for pair, count in collaboration_count.items() if count >= 3]\n",
    "                    if strong_collaborations:\n",
    "                        strong_collaborations.sort(key=lambda x: x[1], reverse=True)\n",
    "                        print(f\\\"\\\\n🤝 Strongest collaborations (3+ papers):\\\")\n",
    "                        for (author1, author2), count in strong_collaborations[:10]:\n",
    "                            print(f\\\"   {author1} ↔ {author2}: {count} papers\\\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\\\"Network analysis error: {e}\\\")\n",
    "            \n",
    "        else:\n",
    "            print(\\\"⚠️  Unable to build collaboration network\\\")\n",
    "    else:\n",
    "        print(\\\"⚠️  No multi-author papers found\\\")\n",
    "    \n",
    "    # Temporal author analysis\n",
    "    if 'year' in df_authors.columns:\n",
    "        author_years = df_authors.dropna(subset=['year'])\n",
    "        if len(author_years) > 0:\n",
    "            # Authors over time\n",
    "            yearly_authors = author_years.groupby('year')['author'].nunique().reset_index()\n",
    "            yearly_authors.columns = ['year', 'unique_authors']\n",
    "            \n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=yearly_authors['year'], \n",
    "                y=yearly_authors['unique_authors'],\n",
    "                mode='lines+markers',\n",
    "                name='Unique Authors',\n",
    "                line=dict(color='darkorange', width=3)\n",
    "            ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=\\\"👥 Research Community Growth Over Time\\\",\n",
    "                xaxis_title=\\\"Year\\\",\n",
    "                yaxis_title=\\\"Number of Unique Authors\\\",\n",
    "                height=400\n",
    "            )\n",
    "            fig.show()\n",
    "            \n",
    "            print(f\\\"\\\\n📈 Author community growth:\\\")\n",
    "            recent_growth = yearly_authors[yearly_authors['year'] >= 2010]\n",
    "            if len(recent_growth) > 1:\n",
    "                growth_rate = (recent_growth['unique_authors'].iloc[-1] - recent_growth['unique_authors'].iloc[0]) / len(recent_growth)\n",
    "                print(f\\\"   Average new authors per year (2010+): {growth_rate:.1f}\\\")\n",
    "            \n",
    "else:\n",
    "    print(\\\"⚠️  No author data available for network analysis\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Research Impact & Quality Analysis\n",
    "\n",
    "print(\"🎯 Analyzing research impact and quality indicators...\")\n",
    "\n",
    "if len(df_enhanced) > 0:\n",
    "    # Research quality indicators\n",
    "    quality_metrics = []\n",
    "    \n",
    "    for _, article in df_enhanced.iterrows():\n",
    "        metrics = {\n",
    "            'pmid': article['pmid'],\n",
    "            'title_length': len(str(article['title'])) if pd.notna(article['title']) else 0,\n",
    "            'abstract_length': len(str(article['abstract'])) if pd.notna(article['abstract']) else 0,\n",
    "            'has_abstract': pd.notna(article['abstract']) and len(str(article['abstract'])) > 10,\n",
    "            'author_count': article['author_count'] if pd.notna(article['author_count']) else 0,\n",
    "            'keyword_count': len(str(article['keywords']).split(';')) if pd.notna(article['keywords']) else 0,\n",
    "            'mesh_count': len(str(article['mesh_terms']).split(';')) if pd.notna(article['mesh_terms']) else 0,\n",
    "            'qac_mentions': article['qac_mention_count'] if pd.notna(article['qac_mention_count']) else 0,\n",
    "            'year': article['year']\n",
    "        }\n",
    "        \n",
    "        # Calculate quality score (0-100)\n",
    "        quality_score = 0\n",
    "        if metrics['has_abstract']: quality_score += 25\n",
    "        if metrics['author_count'] >= 3: quality_score += 15\n",
    "        if metrics['keyword_count'] >= 3: quality_score += 15\n",
    "        if metrics['mesh_count'] >= 3: quality_score += 15\n",
    "        if metrics['abstract_length'] >= 500: quality_score += 10\n",
    "        if metrics['qac_mentions'] >= 2: quality_score += 10\n",
    "        if metrics['title_length'] >= 50: quality_score += 10\n",
    "        \n",
    "        metrics['quality_score'] = quality_score\n",
    "        quality_metrics.append(metrics)\n",
    "    \n",
    "    quality_df = pd.DataFrame(quality_metrics)\n",
    "    \n",
    "    # Create quality analysis dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=('Quality Score Distribution', 'Quality vs. Author Count',\n",
    "                       'Abstract Length Distribution', 'Quality Trends Over Time',\n",
    "                       'QAC Mentions vs. Quality', 'Research Completeness'),\n",
    "        specs=[[{\\\"type\\\": \\\"histogram\\\"}, {\\\"type\\\": \\\"scatter\\\"}],\n",
    "               [{\\\"type\\\": \\\"histogram\\\"}, {\\\"type\\\": \\\"scatter\\\"}],\n",
    "               [{\\\"type\\\": \\\"scatter\\\"}, {\\\"type\\\": \\\"bar\\\"}]]\n",
    "    )\n",
    "    \n",
    "    # Quality score distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=quality_df['quality_score'], nbinsx=20,\n",
    "                    name='Quality Scores', marker_color='skyblue'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Quality vs author count\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=quality_df['author_count'], y=quality_df['quality_score'],\n",
    "                  mode='markers', name='Articles',\n",
    "                  marker=dict(size=6, opacity=0.6, color='coral')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Abstract length distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=quality_df['abstract_length'], nbinsx=30,\n",
    "                    name='Abstract Lengths', marker_color='lightgreen'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Quality trends over time\n",
    "    if quality_df['year'].notna().any():\n",
    "        yearly_quality = quality_df.dropna(subset=['year']).groupby('year')['quality_score'].mean().reset_index()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=yearly_quality['year'], y=yearly_quality['quality_score'],\n",
    "                      mode='lines+markers', name='Avg Quality',\n",
    "                      line=dict(color='purple', width=3)),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # QAC mentions vs quality\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=quality_df['qac_mentions'], y=quality_df['quality_score'],\n",
    "                  mode='markers', name='QAC Focus',\n",
    "                  marker=dict(size=6, opacity=0.6, color='orange')),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # Research completeness\n",
    "    completeness_metrics = {\n",
    "        'Has Abstract': (quality_df['has_abstract'].sum() / len(quality_df)) * 100,\n",
    "        'Multi-Author': (quality_df['author_count'] >= 2).sum() / len(quality_df) * 100,\n",
    "        'Has Keywords': (quality_df['keyword_count'] > 0).sum() / len(quality_df) * 100,\n",
    "        'Has MeSH': (quality_df['mesh_count'] > 0).sum() / len(quality_df) * 100,\n",
    "        'QAC Focused': (quality_df['qac_mentions'] > 0).sum() / len(quality_df) * 100\n",
    "    }\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=list(completeness_metrics.keys()), y=list(completeness_metrics.values()),\n",
    "               name='Completeness %', marker_color='gold'),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=1200, title_text=\\\"📊 Research Quality & Impact Analysis\\\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Quality statistics\n",
    "    print(f\\\"\\\\n📈 Quality Analysis Results:\\\")\n",
    "    print(f\\\"   Average quality score: {quality_df['quality_score'].mean():.1f}/100\\\")\n",
    "    print(f\\\"   High quality articles (>70): {(quality_df['quality_score'] > 70).sum()} ({(quality_df['quality_score'] > 70).mean()*100:.1f}%)\\\")\n",
    "    print(f\\\"   Articles with abstracts: {quality_df['has_abstract'].sum()} ({quality_df['has_abstract'].mean()*100:.1f}%)\\\")\n",
    "    print(f\\\"   Multi-author articles: {(quality_df['author_count'] >= 2).sum()} ({(quality_df['author_count'] >= 2).mean()*100:.1f}%)\\\")\n",
    "    \n",
    "    # Top quality articles\n",
    "    top_quality = quality_df.nlargest(10, 'quality_score')\n",
    "    if len(top_quality) > 0:\n",
    "        print(f\\\"\\\\n🏆 Top 10 highest quality articles:\\\")\n",
    "        for i, (_, row) in enumerate(top_quality.iterrows(), 1):\n",
    "            pmid = row['pmid']\n",
    "            score = row['quality_score']\n",
    "            article_title = df_enhanced[df_enhanced['pmid'] == pmid]['title'].iloc[0] if not df_enhanced[df_enhanced['pmid'] == pmid].empty else \\\"N/A\\\"\n",
    "            print(f\\\"   {i:2d}. PMID {pmid} (Score: {score}): {str(article_title)[:60]}...\\\")\n",
    "    \n",
    "    # Research trends analysis\n",
    "    if quality_df['year'].notna().any():\n",
    "        recent_years = quality_df[quality_df['year'] >= 2015].copy()\n",
    "        older_years = quality_df[quality_df['year'] < 2015].copy()\n",
    "        \n",
    "        if len(recent_years) > 0 and len(older_years) > 0:\n",
    "            print(f\\\"\\\\n📊 Quality evolution:\\\")\n",
    "            print(f\\\"   Pre-2015 average quality: {older_years['quality_score'].mean():.1f}\\\")\n",
    "            print(f\\\"   2015+ average quality: {recent_years['quality_score'].mean():.1f}\\\")\n",
    "            print(f\\\"   Author count evolution: {older_years['author_count'].mean():.1f} → {recent_years['author_count'].mean():.1f}\\\")\n",
    "            print(f\\\"   Abstract length evolution: {older_years['abstract_length'].mean():.0f} → {recent_years['abstract_length'].mean():.0f} chars\\\")\n",
    "    \n",
    "    # Research focus analysis\n",
    "    high_qac_focus = quality_df[quality_df['qac_mentions'] >= 3]\n",
    "    if len(high_qac_focus) > 0:\n",
    "        print(f\\\"\\\\n💊 High QAC-focused research ({len(high_qac_focus)} articles):\\\")\n",
    "        print(f\\\"   Average quality score: {high_qac_focus['quality_score'].mean():.1f}\\\")\n",
    "        print(f\\\"   Average author count: {high_qac_focus['author_count'].mean():.1f}\\\")\n",
    "        print(f\\\"   Average abstract length: {high_qac_focus['abstract_length'].mean():.0f} chars\\\")\n",
    "        \n",
    "else:\n",
    "    print(\\\"⚠️  No article data available for impact analysis\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Comprehensive Analysis Summary & Export\n",
    "\n",
    "print(\"📊 Generating comprehensive analysis summary...\")\n",
    "\n",
    "# Create analysis summary\n",
    "summary = {\n",
    "    'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_summary': {},\n",
    "    'temporal_insights': {},\n",
    "    'content_insights': {},\n",
    "    'collaboration_insights': {},\n",
    "    'quality_insights': {}\n",
    "}\n",
    "\n",
    "if len(df_enhanced) > 0:\n",
    "    # Dataset summary\n",
    "    summary['dataset_summary'] = {\n",
    "        'total_articles': len(df_enhanced),\n",
    "        'articles_with_abstracts': df_enhanced['abstract'].notna().sum(),\n",
    "        'unique_journals': df_enhanced['journal'].nunique() if 'journal' in df_enhanced.columns else 0,\n",
    "        'unique_authors': len(df_authors) if len(df_authors) > 0 else 0,\n",
    "        'total_qac_mentions': len(df_qac_mentions) if len(df_qac_mentions) > 0 else 0,\n",
    "        'year_range': f\\\"{df_enhanced['year'].min():.0f}-{df_enhanced['year'].max():.0f}\\\" if df_enhanced['year'].notna().any() else \\\"N/A\\\"\n",
    "    }\n",
    "    \n",
    "    # Temporal insights\n",
    "    if df_enhanced['year'].notna().any():\n",
    "        yearly_data = df_enhanced.dropna(subset=['year'])\n",
    "        summary['temporal_insights'] = {\n",
    "            'most_productive_year': int(yearly_data.groupby('year').size().idxmax()),\n",
    "            'articles_in_peak_year': int(yearly_data.groupby('year').size().max()),\n",
    "            'articles_last_5_years': len(yearly_data[yearly_data['year'] >= 2020]),\n",
    "            'growth_trend': 'increasing' if yearly_data[yearly_data['year'] >= 2015].groupby('year').size().is_monotonic_increasing else 'variable'\n",
    "        }\n",
    "    \n",
    "    # Content insights\n",
    "    if len(df_qac_mentions) > 0:\n",
    "        top_compound = df_qac_mentions['compound'].value_counts().index[0]\n",
    "        summary['content_insights'] = {\n",
    "            'most_studied_compound': top_compound,\n",
    "            'compound_mentions': int(df_qac_mentions['compound'].value_counts().iloc[0]),\n",
    "            'unique_compounds': df_qac_mentions['compound'].nunique(),\n",
    "            'avg_compounds_per_article': len(df_qac_mentions) / len(df_enhanced)\n",
    "        }\n",
    "    \n",
    "    # Collaboration insights\n",
    "    if len(df_authors) > 0:\n",
    "        summary['collaboration_insights'] = {\n",
    "            'most_prolific_author': df_authors.groupby('author').size().idxmax(),\n",
    "            'author_paper_count': int(df_authors.groupby('author').size().max()),\n",
    "            'avg_authors_per_paper': df_enhanced['author_count'].mean(),\n",
    "            'single_author_papers': (df_enhanced['author_count'] == 1).sum()\n",
    "        }\n",
    "    \n",
    "    # Quality insights\n",
    "    if 'quality_df' in locals():\n",
    "        summary['quality_insights'] = {\n",
    "            'avg_quality_score': quality_df['quality_score'].mean(),\n",
    "            'high_quality_articles': (quality_df['quality_score'] > 70).sum(),\n",
    "            'articles_with_abstracts_pct': (quality_df['has_abstract'].mean() * 100),\n",
    "            'avg_abstract_length': quality_df['abstract_length'].mean()\n",
    "        }\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(\\\"\\\\n\\\" + \\\"=\\\"*80)\n",
    "print(\\\"🎯 COMPREHENSIVE QAC SYSTEMATIC REVIEW ANALYSIS SUMMARY\\\")\n",
    "print(\\\"=\\\"*80)\n",
    "\n",
    "print(f\\\"\\\\n📊 DATASET OVERVIEW\\\")\n",
    "print(f\\\"   Analysis Date: {summary['analysis_date']}\\\")\n",
    "for key, value in summary['dataset_summary'].items():\n",
    "    print(f\\\"   {key.replace('_', ' ').title()}: {value}\\\")\n",
    "\n",
    "if summary['temporal_insights']:\n",
    "    print(f\\\"\\\\n📈 TEMPORAL PATTERNS\\\")\n",
    "    for key, value in summary['temporal_insights'].items():\n",
    "        print(f\\\"   {key.replace('_', ' ').title()}: {value}\\\")\n",
    "\n",
    "if summary['content_insights']:\n",
    "    print(f\\\"\\\\n💊 CONTENT ANALYSIS\\\")\n",
    "    for key, value in summary['content_insights'].items():\n",
    "        print(f\\\"   {key.replace('_', ' ').title()}: {value}\\\")\n",
    "\n",
    "if summary['collaboration_insights']:\n",
    "    print(f\\\"\\\\n🤝 COLLABORATION PATTERNS\\\")\n",
    "    for key, value in summary['collaboration_insights'].items():\n",
    "        print(f\\\"   {key.replace('_', ' ').title()}: {value}\\\")\n",
    "\n",
    "if summary['quality_insights']:\n",
    "    print(f\\\"\\\\n⭐ QUALITY METRICS\\\")\n",
    "    for key, value in summary['quality_insights'].items():\n",
    "        print(f\\\"   {key.replace('_', ' ').title()}: {value:.2f}\\\" if isinstance(value, float) else f\\\"   {key.replace('_', ' ').title()}: {value}\\\")\n",
    "\n",
    "print(\\\"\\\\n\\\" + \\\"=\\\"*80)\n",
    "\n",
    "# Export enhanced datasets\n",
    "print(f\\\"\\\\n💾 Exporting enhanced datasets...\\\")\n",
    "\n",
    "# Export main dataset\n",
    "output_files = []\n",
    "if len(df_enhanced) > 0:\n",
    "    enhanced_filename = \\\"qac_articles_enhanced.csv\\\"\n",
    "    df_enhanced.to_csv(enhanced_filename, index=False, encoding='utf-8')\n",
    "    output_files.append(enhanced_filename)\n",
    "    print(f\\\"✅ Exported enhanced articles: {enhanced_filename}\\\")\n",
    "\n",
    "# Export QAC mentions\n",
    "if len(df_qac_mentions) > 0:\n",
    "    qac_filename = \\\"qac_compound_mentions.csv\\\"\n",
    "    df_qac_mentions.to_csv(qac_filename, index=False, encoding='utf-8')\n",
    "    output_files.append(qac_filename)\n",
    "    print(f\\\"✅ Exported QAC mentions: {qac_filename}\\\")\n",
    "\n",
    "# Export author data\n",
    "if len(df_authors) > 0:\n",
    "    authors_filename = \\\"qac_authors.csv\\\"\n",
    "    df_authors.to_csv(authors_filename, index=False, encoding='utf-8')\n",
    "    output_files.append(authors_filename)\n",
    "    print(f\\\"✅ Exported author data: {authors_filename}\\\")\n",
    "\n",
    "# Export journal analysis\n",
    "if len(df_journals) > 0:\n",
    "    journals_filename = \\\"qac_journals.csv\\\"\n",
    "    df_journals.to_csv(journals_filename, index=False, encoding='utf-8')\n",
    "    output_files.append(journals_filename)\n",
    "    print(f\\\"✅ Exported journal data: {journals_filename}\\\")\n",
    "\n",
    "# Export summary report\n",
    "summary_filename = \\\"qac_analysis_summary.json\\\"\n",
    "import json\n",
    "with open(summary_filename, 'w', encoding='utf-8') as f:\n",
    "    # Convert numpy types to Python native types for JSON serialization\n",
    "    def convert_numpy(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        return obj\n",
    "    \n",
    "    # Deep convert the summary\n",
    "    def deep_convert(item):\n",
    "        if isinstance(item, dict):\n",
    "            return {k: deep_convert(v) for k, v in item.items()}\n",
    "        elif isinstance(item, list):\n",
    "            return [deep_convert(v) for v in item]\n",
    "        else:\n",
    "            return convert_numpy(item)\n",
    "    \n",
    "    json.dump(deep_convert(summary), f, indent=2, default=str)\n",
    "\n",
    "output_files.append(summary_filename)\n",
    "print(f\\\"✅ Exported analysis summary: {summary_filename}\\\")\n",
    "\n",
    "# Create analysis report\n",
    "report_filename = \\\"QAC_Analysis_Report.md\\\"\n",
    "with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(\\\"# QAC Systematic Review Analysis Report\\\\n\\\\n\\\")\n",
    "    f.write(f\\\"**Generated:** {summary['analysis_date']}\\\\n\\\\n\\\")\n",
    "    \n",
    "    f.write(\\\"## Executive Summary\\\\n\\\\n\\\")\n",
    "    f.write(f\\\"This analysis examined **{summary['dataset_summary']['total_articles']} articles** related to Quaternary Ammonium Compounds (QACs) from the PubMed database. \\\")\n",
    "    \n",
    "    if summary['temporal_insights']:\n",
    "        f.write(f\\\"The research spans from {summary['dataset_summary']['year_range']}, with peak activity in {summary['temporal_insights']['most_productive_year']} ({summary['temporal_insights']['articles_in_peak_year']} articles). \\\")\n",
    "    \n",
    "    if summary['content_insights']:\n",
    "        f.write(f\\\"A total of {summary['content_insights']['total_qac_mentions']} QAC compound mentions were identified across {summary['content_insights']['unique_compounds']} unique compounds.\\\\n\\\\n\\\")\n",
    "    \n",
    "    f.write(\\\"## Key Findings\\\\n\\\\n\\\")\n",
    "    \n",
    "    if summary['temporal_insights']:\n",
    "        f.write(\\\"### Temporal Trends\\\\n\\\")\n",
    "        f.write(f\\\"- Research activity shows a **{summary['temporal_insights']['growth_trend']}** trend\\\\n\\\")\n",
    "        f.write(f\\\"- Most productive year: **{summary['temporal_insights']['most_productive_year']}** ({summary['temporal_insights']['articles_in_peak_year']} articles)\\\\n\\\")\n",
    "        f.write(f\\\"- Recent activity (2020+): **{summary['temporal_insights']['articles_last_5_years']} articles**\\\\n\\\\n\\\")\n",
    "    \n",
    "    if summary['content_insights']:\n",
    "        f.write(\\\"### Research Focus\\\\n\\\")\n",
    "        f.write(f\\\"- Most studied compound: **{summary['content_insights']['most_studied_compound']}** ({summary['content_insights']['compound_mentions']} mentions)\\\\n\\\")\n",
    "        f.write(f\\\"- Average QAC mentions per article: **{summary['content_insights']['avg_compounds_per_article']:.2f}**\\\\n\\\\n\\\")\n",
    "    \n",
    "    if summary['collaboration_insights']:\n",
    "        f.write(\\\"### Collaboration Patterns\\\\n\\\")\n",
    "        f.write(f\\\"- Most prolific author: **{summary['collaboration_insights']['most_prolific_author']}** ({summary['collaboration_insights']['author_paper_count']} papers)\\\\n\\\")\n",
    "        f.write(f\\\"- Average authors per paper: **{summary['collaboration_insights']['avg_authors_per_paper']:.1f}**\\\\n\\\")\n",
    "        f.write(f\\\"- Single-author papers: **{summary['collaboration_insights']['single_author_papers']}**\\\\n\\\\n\\\")\n",
    "    \n",
    "    if summary['quality_insights']:\n",
    "        f.write(\\\"### Research Quality\\\\n\\\")\n",
    "        f.write(f\\\"- Average quality score: **{summary['quality_insights']['avg_quality_score']:.1f}/100**\\\\n\\\")\n",
    "        f.write(f\\\"- High-quality articles (>70): **{summary['quality_insights']['high_quality_articles']}**\\\\n\\\")\n",
    "        f.write(f\\\"- Articles with abstracts: **{summary['quality_insights']['articles_with_abstracts_pct']:.1f}%**\\\\n\\\\n\\\")\n",
    "    \n",
    "    f.write(\\\"## Data Files\\\\n\\\\n\\\")\n",
    "    f.write(\\\"The following data files were generated:\\\\n\\\\n\\\")\n",
    "    for file in output_files:\n",
    "        f.write(f\\\"- `{file}`\\\\n\\\")\n",
    "    \n",
    "    f.write(\\\"\\\\n---\\\\n\\\")\n",
    "    f.write(\\\"*Report generated using automated QAC systematic review analysis pipeline*\\\\n\\\")\n",
    "\n",
    "output_files.append(report_filename)\n",
    "print(f\\\"✅ Generated analysis report: {report_filename}\\\")\n",
    "\n",
    "print(f\\\"\\\\n🎉 Analysis complete! Generated {len(output_files)} output files:\\\")\n",
    "for file in output_files:\n",
    "    print(f\\\"   📄 {file}\\\")\n",
    "\n",
    "print(f\\\"\\\\n🚀 Your QAC systematic review analysis is ready for further investigation!\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
